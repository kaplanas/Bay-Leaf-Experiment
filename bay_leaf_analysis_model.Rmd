---
title: 'The Great Bay Leaf Experiment: Model'
author: "Abby Kaplan"
date: "November 12, 2018"
output: html_document
---

```{r setup, echo = F, message = F, warning = F}
library(knitr)
library(kableExtra)
library(tibble)
library(tidyr)
library(dplyr)
library(arm)
library(rstan)
library(ggplot2)
theme_set(theme_bw())
```

## Background

This page is for people who are interested in the technical details of the statistical model.  Click [here](bay_leaf_analysis.html) for background on what the study is all about.

## The model

### Outcome and parameters

The outcome variable, $\mbox{prefer.bay.leaf}$, is binary.  Let's do a logistic regression.  In addition to the overall intercept, we'll have adjustments to the intercept by batch, subject, and pair.

$$\mbox{prefer.bay.leaf} = \mbox{Bernoulli}(\mbox{logit}^{-1}(\beta + \gamma_i\sigma_{\gamma_i} + \delta_j\sigma_{\delta_j} + \epsilon_k\sigma_{\epsilon_k}))$$

* $\beta_0$ is the intercept: the overall preference for portions with bay leaf.  This is the parameter we're most interested in.
* $\gamma_i$ is the adjustment to the intercept for batch $i$.
* $\delta_j$ is the adjustment to the intercept for subject $j$, nested within batch $i$.
* $\epsilon_k$ is the adjustment to the intercept for pair $k$, nested within batch $i$.

(In real life, subjects weren't actually nested within batches; some subjects -- including me -- participated in multiple batches.  But, to preserve subjects' privacy, I haven't tracked subject identity across batches.)

### Priors

I think it's unlikely that people will prefer bay leaf (or lack of bay leaf) as often as 90% of the time; this suggests that $\mbox{abs}(\beta_0)$ should be less than `r round(logit(0.9), 2)`.  A prior of $\mbox{N}(0, 1)$ for $\beta_0$ seems about right.

$$\beta_0 \sim \mbox{N}(0, 1)$$

I'm including a parameter for the standard deviation of each of the group-level intercepts, and giving each of these $\sigma$ parameters a half-Cauchy prior.

Priors for batches:

$$\sigma_\gamma \in [0, \inf] \sim \mbox{Cauchy}(0, 1)$$

$$\gamma_i \sim \mbox{N}(0, 1)$$

Priors for pairs:

$$\sigma_\delta \in [0, \inf] \sim \mbox{Cauchy}(0, 1)$$

$$\delta_j \sim \mbox{N}(0, 1)$$

Priors for subjects:

$$\sigma_\epsilon \in [0, \inf] \sim \mbox{Cauchy}(0, 1)$$

$$\epsilon_k \sim \mbox{N}(0, 1)$$

## Simulated data

To test the specification of the model, let's simulate some data and test whether we can recover the true parameters.

### Simulated inputs

For a large dataset, I would create a much smaller simulated dataset with inputs that have approximately the same distribution as the inputs in the full dataset.  But we're working with much smaller amounts of data here, so I'll just copy the actual inputs.

```{r simulate_inputs, eval = F}
sim.df = obs.df[,c("obs.id", "int.batch.id", "int.subject.id", "int.pair.id")]
```

### Simulated parameters

```{r simulate_parameters, eval = F}
# Simulate parameters by drawing from their priors.
true.params.sim.df = data.frame(
  draw.id = 1,
  beta = rnorm(1, 0, 1),
  sigma_gamma = abs(rcauchy(1, 0, 1)),
  sigma_delta = abs(rcauchy(1, 0, 1)),
  sigma_epsilon = abs(rcauchy(1, 0, 1))
)
true.params.sim.df[,paste("gamma.", sort(unique(obs.df$int.batch.id)), ".", sep = "")] =
  rnorm(length(unique(obs.df$int.batch.id)), 0, 1)
true.params.sim.df[,paste("delta.", sort(unique(obs.df$int.subject.id)), ".", sep = "")] =
  rnorm(length(unique(obs.df$int.subject.id)), 0, 1)
true.params.sim.df[,paste("epsilon.", sort(unique(obs.df$int.pair.id)), ".", sep = "")] =
  rnorm(length(unique(obs.df$int.pair.id)), 0, 1)
true.params.sim.long.df = true.params.sim.df %>%
  gather(parameter, true.value)
```

### Simulated outcomes

```{r simulate_outcomes, eval = F}
# A utility function that simulates outcomes for a given set of parameter
# draws; we'll use this both to generate outcomes for the simulated dataset and
# to do posterior predictive checking.
generate.outcomes = function(inputs.df, parameters.df, n.draws) {
  # Set up the dataframe to hold both inputs and generated outcomes.
  outputs.df = inputs.df
  # Create model matrices for batch, subject, and pair.
  mm.batch = model.matrix(~ factor(int.batch.id), outputs.df,
                          contrasts.arg =
                            list("factor(int.batch.id)" =
                                   contrasts(factor(outputs.df$int.batch.id),
                                             contrasts = F)))[,-1]
  mm.subject = model.matrix(~ factor(int.subject.id), outputs.df,
                            contrasts.arg =
                              list("factor(int.subject.id)" =
                                   contrasts(factor(outputs.df$int.subject.id),
                                             contrasts = F)))[,-1]
  mm.pair = model.matrix(~ factor(int.pair.id), outputs.df,
                         contrasts.arg =
                           list("factor(int.pair.id)" =
                                  contrasts(factor(outputs.df$int.pair.id),
                                            contrasts = F)))[,-1]
  # Randomly draw from the parameters the specified number of times.
  draws.df = sample_n(parameters.df, n.draws, replace = T)
  # Generate the probability of preferring bay leaf for each observation and
  # for each draw.
  prefer.bay.leaf.prob.pred =
    invlogit(
      cbind(rep(1, nrow(outputs.df)),
            mm.batch,
            mm.subject,
            mm.pair) %*%
        t(cbind(draws.df$beta,
                draws.df[,paste("gamma.",
                                sort(unique(outputs.df$int.batch.id)),
                                ".", sep = "")] * draws.df$sigma_gamma,
                draws.df[,paste("delta.",
                                sort(unique(outputs.df$int.subject.id)),
                                ".", sep = "")] * draws.df$sigma_delta,
                draws.df[,paste("epsilon.",
                                sort(unique(outputs.df$int.pair.id)),
                                ".", sep = "")] * draws.df$sigma_epsilon))
    )
  # Convert each probability to a prediction (i.e., sample from the
  # probability) and add the predictions to the data frame.
  prefer.bay.leaf.preds = matrix(ifelse(prefer.bay.leaf.prob.pred >
                                          runif(length(prefer.bay.leaf.prob.pred),
                                                       0, 1), 1, 0),
                                 ncol = ncol(prefer.bay.leaf.prob.pred))
  outputs.df = cbind(outputs.df,
                     prefer.bay.leaf.preds)
  # Convert the dataframe from wide to long.
  outputs.df = outputs.df %>%
    gather(draw.id, prefer.bay.leaf.pred, -obs.id, -int.batch.id, -int.subject.id, -int.pair.id) %>%
    dplyr::select(obs.id, int.batch.id, int.subject.id, int.pair.id, prefer.bay.leaf.pred)
  return(outputs.df)
}
# Use the function to generate simulated outcomes.
sim.df = generate.outcomes(sim.df, true.params.sim.df, 1) %>%
  mutate(prefer.bay.leaf = prefer.bay.leaf.pred == 1) %>%
  rename(int.prefer.bay.leaf = prefer.bay.leaf.pred)
```

### Fit the simulated data

```{r fit_sim_data, eval = F, message = F, warning = F}
# Create the dataset for Stan.
sim.data.for.stan = list(N = nrow(sim.df),
                         I = max(sim.df$int.batch.id),
                         J = max(sim.df$int.subject.id),
                         K = max(sim.df$int.pair.id),
                         batch = sim.df$int.batch.id,
                         subject = sim.df$int.subject.id,
                         pair = sim.df$int.pair.id,
                         prefer_bay_leaf = sim.df$int.prefer.bay.leaf)
# Fit the model.
fit.sim.stan = stan("bay_leaf_analysis.stan",
                    data = sim.data.for.stan,
                    chains = 4, iter = 2000,
                    control = list(adapt_delta = 0.99))
```

### Check the fit of the simulated data

There were no divergent transitions.  The chains are stationary and well mixed.

```{r plot_sim_chains, message = F, warning = F, fig.width = 12, fig.height = 12}
stan_trace(fit.sim.stan,
           pars = c("beta", "sigma_gamma", "gamma", "sigma_delta", "delta",
                    "sigma_epsilon", "epsilon"))
```

### Did the model capture the true parameter values?

```{r get_sampled_sim_parameters, message = F, warning = F}
# Extract the sampled parameter values from the model fit.
sampled.params.sim.df = as.data.frame(fit.sim.stan) %>%
  setNames(gsub("[[]|[]]", "\\.", colnames(.)))
sampled.params.sim.long.df = sampled.params.sim.df %>%
  gather(parameter, sampled.value)
```

The model has done a decent job of capturing the true values of $\beta$, $\sigma_\gamma$, $\sigma_\delta$, and $\sigma_\epsilon$ (plotted in red).  But there's a huge amount of spread in the sampled parameter values, probably because we just don't have much data.  The effect of bay leaf would have to be very large in order for us to detect it with confidence.

```{r plot_sampled_sim_parameters_beta_sigma, message = F, warning = F, fig.width = 6, fig.height = 4}
sampled.params.sim.long.df %>%
  filter(grepl("beta|sigma", parameter)) %>%
  ggplot(aes(x = sampled.value)) +
  geom_density(fill = "lightgray", size = 0.3) +
  geom_vline(data = true.params.sim.long.df %>%
               filter(grepl("beta|sigma", parameter)),
             aes(xintercept = true.value),
             col = "red") +
  facet_wrap(~ parameter, scale = "free") +
  scale_x_continuous("sampled values") +
  scale_y_continuous("")
```

<a name="simfit"></a>The model does less well in recovering the group-level intercepts $\gamma_i$, $\delta_j$, and $\epsilon_k$.  This isn't shocking, since we have so few observations for each group.  In fact, the recovered parameter values are so unreliable that I'm reluctant to try to do much in the way of inference on the real data at all.  At best, if the samples for a given parameter are reliably on one side of zero, we might be able to conclude that there's a real effect of bay leaf in that group.  But *lack* of an effect should be interpreted with extreme caution, because of the wide credible intervals and because of the shrinkage toward zero that we see below.

```{r plot_sampled_sim_group_parameters, message = F, warning = F, fig.width = 8, fig.height = 3}
sampled.params.sim.long.df %>%
  filter(grepl("^(gamma|delta|epsilon)", parameter)) %>%
  mutate(parameter.group = gsub("^(gamma|delta|epsilon).*", "\\1", parameter)) %>%
  group_by(parameter.group, parameter) %>%
  summarize(mean.sampled.value = mean(sampled.value),
            upper.bound = quantile(sampled.value, 0.975),
            lower.bound = quantile(sampled.value, 0.025)) %>%
  inner_join(true.params.sim.long.df, by = c("parameter")) %>%
  ggplot(aes(x = true.value, y = mean.sampled.value)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower.bound, ymax = upper.bound)) +
  geom_abline(intercept = 0, slope = 1, col = "gray") +
  facet_wrap(~ parameter.group) +
  scale_x_continuous("true value") +
  scale_y_continuous("sampled values")
```

### Posterior predictive checks

Overall, the model is somewhat underpredicting the odds of observations at both extremes; basically, its estimates are a little too conservative.  But given the small dataset, I think this isn't too bad.

```{r posterior_predictive_checks_sim, message = F, warning = F, fig.width = 6, fig.height = 4}
# Generate predicted outputs for the simulated data based on the fitted model.
sim.pred.df = generate.outcomes(sim.df,
                                sampled.params.sim.df,
                                1000) %>%
  group_by(obs.id) %>%
  summarize(mean.pred = mean(prefer.bay.leaf.pred)) %>%
  inner_join(sim.df, by = c("obs.id"))
ggplot(sim.pred.df, aes(x = mean.pred, y = int.prefer.bay.leaf)) +
  geom_jitter(height = 0.3, width = 0) +
  geom_abline(intercept = 0, slope = 1, col = "gray") +
  stat_smooth() +
  scale_x_continuous("mean predicted preference for bay leaf") +
  scale_y_continuous("actual choice")
```

## Fit the real data

```{r fit_real_data, eval = F, message = F, warning = F}
# Create the dataset for Stan.
data.for.stan = list(N = nrow(obs.df),
                     I = max(obs.df$int.batch.id),
                     J = max(obs.df$int.subject.id),
                     K = max(obs.df$int.pair.id),
                     batch = obs.df$int.batch.id,
                     subject = obs.df$int.subject.id,
                     pair = obs.df$int.pair.id,
                     prefer_bay_leaf = obs.df$int.prefer.bay.leaf)
# Fit the model.
fit.stan = stan("bay_leaf_analysis.stan",
                data = data.for.stan,
                chains = 4, iter = 2000,
                control = list(adapt_delta = 0.99))
```

### Check the fit of the model

No divergent transitions.  The chains look great.

```{r plot_chains, echo = F, message = F, warning = F, fig.width = 12, fig.height = 8}
stan_trace(fit.stan,
           pars = c("beta", "sigma_gamma", "gamma", "sigma_delta", "delta",
                    "sigma_epsilon", "epsilon"))
```

### Posterior predictive checks

The posterior predictions are...not great.  But it's worth noticing that the fit for the real data has fewer extreme predictions than the fit for the simulated data.  Could it be that we're just seeing a model that, much of the time, correctly predicts "meh"?

```{r posterior_predictive_checks, message = F, warning = F, fig.width = 6, fig.height = 4}
# Extract the sampled parameter values from the model fit.
sampled.params.df = as.data.frame(fit.stan) %>%
  setNames(gsub("[[]|[]]", "\\.", colnames(.))) %>%
  rownames_to_column("draw.id")
sampled.params.long.df = sampled.params.df %>%
  gather(parameter, sampled.value, -draw.id) %>%
  mutate(parameter.group =  gsub("^.*(beta|gamma|delta|epsilon).*$", "\\1", parameter),
         parameter.num = as.numeric(gsub("^.*\\.([0-9]+)\\..*$", "\\1", parameter)))
# Generate predicted outputs for the real data based on the fitted model.
pred.df = generate.outcomes(obs.df,
                            sampled.params.df,
                            1000) %>%
  group_by(obs.id) %>%
  summarize(mean.pred = mean(prefer.bay.leaf.pred)) %>%
  inner_join(sim.df, by = c("obs.id"))
ggplot(pred.df, aes(x = mean.pred, y = int.prefer.bay.leaf)) +
  geom_jitter(height = 0.3, width = 0) +
  geom_abline(intercept = 0, slope = 1, col = "gray") +
  stat_smooth() +
  scale_x_continuous("mean predicted preference for bay leaf") +
  scale_y_continuous("actual choice")
```

